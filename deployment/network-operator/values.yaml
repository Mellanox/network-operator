# Copyright 2020 NVIDIA
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for network-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nfd:
  enabled: true
  deployNodeFeatureRules: true

psp:
  enabled: false

upgradeCRDs: true

sriovNetworkOperator:
  enabled: false
  # inject additional values to nodeSelector for config daemon
  configDaemonNodeSelectorExtra:
    node-role.kubernetes.io/worker: ""

# Node Feature discovery chart related values
node-feature-discovery:
  enableNodeFeatureApi: true
  worker:
    serviceAccount:
      name: node-feature-discovery
      # disable creation to avoid duplicate serviceaccount creation by master spec below
      create: false
    tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    config:
      sources:
        pci:
          deviceClassWhitelist:
          - "0300"
          - "0302"
          deviceLabelFields:
          - vendor
  master:
    serviceAccount:
      name: node-feature-discovery
      create: true
    config: 
      extraLabelNs: ["nvidia.com"]

# SR-IOV Network Operator chart related values
sriov-network-operator:
  operator:
    tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
    nodeSelector: {}
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: In
                  values: [""]
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: In
                  values: [ "" ]
    nameOverride: ""
    fullnameOverride: ""
    resourcePrefix: "nvidia.com"
    enableAdmissionController: false
    cniBinPath: "/opt/cni/bin"
    clusterType: "kubernetes"

  # Image URIs for sriov-network-operator components
  images:
    operator: nvcr.io/nvidia/mellanox/sriov-network-operator:network-operator-23.7.0
    sriovConfigDaemon: nvcr.io/nvidia/mellanox/sriov-network-operator-config-daemon:network-operator-23.7.0
    sriovCni: ghcr.io/k8snetworkplumbingwg/sriov-cni:v2.7.0
    ibSriovCni:  ghcr.io/k8snetworkplumbingwg/ib-sriov-cni:v1.0.3
    sriovDevicePlugin: ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin:7e7f979087286ee950bd5ebc89d8bbb6723fc625
    resourcesInjector: ghcr.io/k8snetworkplumbingwg/network-resources-injector:v1.4
    webhook: ghcr.io/k8snetworkplumbingwg/sriov-network-operator-webhook:v1.1.0
  # imagePullSecrest for SR-IOV Network Operator related images
  # imagePullSecrets: []

# General Operator related values
# The operator element allows to deploy network operator from an alternate location
operator:
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: In
                values: [""]
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/control-plane"
                operator: In
                values: [ "" ]
  repository: nvcr.io/nvidia/cloud-native
  image: network-operator
  # imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  # tag, if defined will use the given image tag, else Chart.AppVersion will be used
  # tag

imagePullSecrets: []

# NicClusterPolicy CR values:
deployCR: false
ofedDriver:
  deploy: false
  image: mofed
  repository: nvcr.io/nvidia/mellanox
  version: 23.07-0.5.0.0
  # imagePullSecrets: []
  # env, if defined will pass environment variables to the OFED container
  # env:
  #   - name: EXAMPLE_ENV_VAR
  #     value: example_env_var_value
  terminationGracePeriodSeconds: 300
  # Private mirror repository configuration
  repoConfig:
    name: ""
  # Custom ssl key/certificate configuration
  certConfig:
    name: ""

  startupProbe:
    initialDelaySeconds: 10
    periodSeconds: 20
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 30
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 30
  upgradePolicy:
    # global switch for automatic upgrade feature
    # if set to false all other options are ignored
    autoUpgrade: false
    # how many nodes can be upgraded in parallel (default: 1)
    # 0 means no limit, all nodes will be upgraded in parallel
    maxParallelUpgrades: 1
    # options for node drain (`kubectl drain`) before the driver reload
    # if auto upgrade is enabled but drain.enable is false,
    # then driver POD will be reloaded immediately without
    # removing PODs from the node
    drain:
      enable: true
      force: false
      podSelector: ""
      # It's recommended to set a timeout to avoid infinite drain in case non-fatal error keeps happening on retries
      timeoutSeconds: 300
      deleteEmptyDir: false

rdmaSharedDevicePlugin:
  deploy: true
  image: k8s-rdma-shared-dev-plugin
  repository: nvcr.io/nvidia/cloud-native
  version: v1.3.2
  # imagePullSecrets: []
  # The following defines the RDMA resources in the cluster
  # it must be provided by the user when deploying the chart
  # each entry in the resources element will create a resource with the provided <name> and list of devices
  # example:
  resources:
    - name: rdma_shared_device_a
      vendors: [15b3]
      rdmaHcaMax: 63

sriovDevicePlugin:
  deploy: false
  image: sriov-network-device-plugin
  repository: ghcr.io/k8snetworkplumbingwg
  version: 7e7f979087286ee950bd5ebc89d8bbb6723fc625
  # imagePullSecrets: []
  resources:
    - name: hostdev
      vendors: [15b3]

ibKubernetes:
  deploy: false
  image: ib-kubernetes
  repository: ghcr.io/mellanox
  version: v1.0.2
  # imagePullSecrets: []
  periodicUpdateSeconds: 5
  pKeyGUIDPoolRangeStart: "02:00:00:00:00:00:00:00"
  pKeyGUIDPoolRangeEnd: "02:FF:FF:FF:FF:FF:FF:FF"
  ufmSecret: '' # specify the secret name here

nvIpam:
  deploy: false
  image: nvidia-k8s-ipam
  repository: ghcr.io/mellanox
  version: v0.0.3
  # imagePullSecrets: []
  # network pool configuration as described in https://github.com/Mellanox/nvidia-k8s-ipam
  config: |-
    {
    "pools":  {
      "rdma-pool": {"subnet": "192.168.0.0/16", "perNodeBlockSize": 100, "gateway": "192.168.0.1"}
      }
    }

secondaryNetwork:
  deploy: true
  cniPlugins:
    deploy: true
    image: plugins
    repository: ghcr.io/k8snetworkplumbingwg
    version: v1.2.0-amd64
    # imagePullSecrets: []
  multus:
    deploy: true
    image: multus-cni
    repository: ghcr.io/k8snetworkplumbingwg
    version: v3.9.3
    # imagePullSecrets: []
    # config: ''
  ipoib:
    deploy: false
    image: ipoib-cni
    repository: nvcr.io/nvidia/cloud-native
    version: v1.1.0
    # imagePullSecrets: []
  ipamPlugin:
    deploy: true
    image: whereabouts
    repository: ghcr.io/k8snetworkplumbingwg
    version: v0.6.1-amd64
    # imagePullSecrets: []

# Can be set to nicclusterpolicy and override other ds node affinity,
# e.g. https://github.com/Mellanox/network-operator/blob/master/manifests/state-multus-cni/0050-multus-ds.yml#L26-L36
#nodeAffinity:

# Can be set to nicclusterpolicy to add extra tolerations to ds
#tolerations:

test:
  pf: ens2f0

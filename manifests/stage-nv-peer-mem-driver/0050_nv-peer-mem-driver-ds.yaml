# Copyright 2020 NVIDIA
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
  name: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}-ds
  namespace: {{ .RuntimeSpec.Namespace }}
spec:
  selector:
    matchLabels:
      app: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
  template:
    metadata:
      labels:
        app: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
    spec:
      priorityClassName: system-node-critical
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
{{if eq .RuntimeSpec.OSName "rhcos"}}
      serviceAccountName: nv-peer-mem-driver
{{end}}
      hostNetwork: true
      {{- if .CrSpec.ImagePullSecrets }}
      imagePullSecrets:
      {{- range .CrSpec.ImagePullSecrets }}
        - name: {{ . }}
      {{- end }}
      {{- end }}
      initContainers:
      - name: gpu-driver-validation
        image: {{ .CrSpec.Repository }}/{{ .CrSpec.Image }}-{{ .CrSpec.Version }}:{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c']
        args: ["export SYS_LIBRARY_PATH=$(ldconfig -v 2>/dev/null | grep -v '^[[:space:]]' | cut -d':' -f1 | tr '[[:space:]]' ':'); \
        export NVIDIA_LIBRARY_PATH=/run/nvidia/drivers/usr/lib/x86_64-linux-gnu/:/run/nvidia/drivers/usr/lib64; \
        export LD_LIBRARY_PATH=${SYS_LIBRARY_PATH}:${NVIDIA_LIBRARY_PATH}; echo ${LD_LIBRARY_PATH}; \
        export PATH=/run/nvidia/drivers/usr/bin/:${PATH}; \
        until nvidia-smi; do echo waiting for nvidia drivers to be loaded; sleep 5; done"]
        securityContext:
          privileged: true
          seLinuxOptions:
            level: "s0"
        volumeMounts:
          - name: run-nvidia
            mountPath: /run/nvidia/drivers
            mountPropagation: HostToContainer
      containers:
        - image: {{ .CrSpec.Repository }}/{{ .CrSpec.Image }}-{{ .CrSpec.Version }}:{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
          imagePullPolicy: IfNotPresent
          name: nv-peer-mem-driver-container
          securityContext:
            privileged: true
            seLinuxOptions:
              level: "s0"
          env:
            - name: HTTP_PROXY
              value: {{ .RuntimeSpec.HTTPProxy }}
            - name: HTTPS_PROXY
              value: {{ .RuntimeSpec.HTTPSProxy }}
            - name: http_proxy
              value: {{ .RuntimeSpec.HTTPProxy }}
            - name: https_proxy
              value: {{ .RuntimeSpec.HTTPSProxy }}
            - name: NO_PROXY
              value: {{ .RuntimeSpec.NoProxy }}
          volumeMounts:
            - name: mlnx-ofed-usr-src
              mountPath: /run/mellanox/drivers/usr/src
              mountPropagation: HostToContainer
            - name: mlnx-ofed-lib-modules
              mountPath: /run/mellanox/drivers/lib/modules
              mountPropagation: HostToContainer
            - name: run-nvidia
              mountPath: /run/nvidia/drivers
              mountPropagation: HostToContainer
          startupProbe:
            exec:
              command:
                [sh, -c, 'ls /.driver-ready']
            initialDelaySeconds: 10
            failureThreshold: 60
            successThreshold: 1
            periodSeconds: 5
          livenessProbe:
            exec:
              command:
                [sh, -c, 'lsmod | grep nv_peer_mem']
            periodSeconds: 30
            initialDelaySeconds: 30
            failureThreshold: 1
            successThreshold: 1
          readinessProbe:
            exec:
              command:
                [sh, -c, 'lsmod | grep nv_peer_mem']
            periodSeconds: 30
            initialDelaySeconds: 10
            failureThreshold: 1
      volumes:
        {{ $ofedPath := "/run/mellanox/drivers" }}
        {{ if .RuntimeSpec.UseHostOFED }}
          {{ $ofedPath = "" }}
        {{end}}
        - name: mlnx-ofed-usr-src
          hostPath:
            path: {{ $ofedPath }}/usr/src
        - name: mlnx-ofed-lib-modules
          hostPath:
            path: {{ $ofedPath }}/lib/modules
        - name: run-nvidia
          hostPath:
            path: {{ .CrSpec.GPUDriverSourcePath }}
      nodeSelector:
        feature.node.kubernetes.io/pci-15b3.present: "true"
        nvidia.com/gpu.present: "true"
        network.nvidia.com/operator.mofed.wait: "false"
      # This code will be dropped when network operator stops supporting nv-peer-mem-driver
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.count
                operator: Gt
                values:
                  - "0"
              # cuda driver starting from v465 should include builtin nvidia_peermem module
              # which should be used instead nv_peer_mem
              # prevent scheduling nv-peer-mem-driver POD on nodes with cuda driver version >= 465
              - key: nvidia.com/cuda.driver.major
                operator: Lt
                values:
                  - "{{ .RuntimeSpec.MaxCudaVersion }}"
        # Above there are mandatory keys for nv-peer-mem-driver affinity,
        # so, in order to apply user-defined node affinity, we have to merge with those keys and patch nodeAffinity obj
        {{- if .NodeAffinity }}
          {{- if .NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution }}
            {{- if (len .NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms) }}
              {{- range $term := .NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms }}
              {{- range $term.MatchExpressions }}
              {{- . | yaml | nindentPrefix 16 "- " }}
              {{- end }}
              {{- end }}
            {{- end}}
          {{- end }}
          {{- if .NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution }}
          preferredDuringSchedulingIgnoredDuringExecution:
            {{- .NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution | yaml | nindent 12 }}
          {{- end }}
        {{- end }}

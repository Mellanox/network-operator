# Copyright 2020 NVIDIA
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
  name: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}-ds
  namespace: {{ .RuntimeSpec.Namespace }}
spec:
  selector:
    matchLabels:
      app: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
  template:
    metadata:
      # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
      # reserves resources for critical add-on pods so that they can be rescheduled after
      # a failure.  This annotation works in tandem with the toleration below.
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        app: nv-peer-mem-driver-{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
    spec:
      tolerations:
        # Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
        # This, along with the annotation above marks this pod as a critical add-on.
        - key: CriticalAddonsOnly
          operator: Exists
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
{{if eq .RuntimeSpec.OSName "rhcos"}}
      serviceAccountName: nv-peer-mem-driver
{{end}}
      hostNetwork: true
      {{- if .CrSpec.ImagePullSecrets }}
      imagePullSecrets:
      {{- range .CrSpec.ImagePullSecrets }}
        - name: {{ . }}
      {{- end }}
      {{- end }}
      initContainers:
      - name: gpu-driver-validation
        image: {{ .CrSpec.Repository }}/{{ .CrSpec.Image }}-{{ .CrSpec.Version }}:{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c']
        args: ["export SYS_LIBRARY_PATH=$(ldconfig -v 2>/dev/null | grep -v '^[[:space:]]' | cut -d':' -f1 | tr '[[:space:]]' ':'); \
        export NVIDIA_LIBRARY_PATH=/run/nvidia/drivers/usr/lib/x86_64-linux-gnu/:/run/nvidia/drivers/usr/lib64; \
        export LD_LIBRARY_PATH=${SYS_LIBRARY_PATH}:${NVIDIA_LIBRARY_PATH}; echo ${LD_LIBRARY_PATH}; \
        export PATH=/run/nvidia/drivers/usr/bin/:${PATH}; \
        until nvidia-smi; do echo waiting for nvidia drivers to be loaded; sleep 5; done"]
        securityContext:
          privileged: true
          seLinuxOptions:
            level: "s0"
        volumeMounts:
          - name: run-nvidia
            mountPath: /run/nvidia/drivers
            mountPropagation: HostToContainer
      containers:
        - image: {{ .CrSpec.Repository }}/{{ .CrSpec.Image }}-{{ .CrSpec.Version }}:{{ .RuntimeSpec.CPUArch }}-{{ .RuntimeSpec.OSName }}{{ .RuntimeSpec.OSVer }}
          imagePullPolicy: IfNotPresent
          name: nv-peer-mem-driver-container
          securityContext:
            privileged: true
            seLinuxOptions:
              level: "s0"
          env:
            - name: HTTP_PROXY
              value: {{ .RuntimeSpec.HTTPProxy }}
            - name: HTTPS_PROXY
              value: {{ .RuntimeSpec.HTTPSProxy }}
            - name: NO_PROXY
              value: {{ .RuntimeSpec.NoProxy }}
            - name: ALLOW_MOD_FROM_GPU_DRIVER
              value: '{{ .CrSpec.FromGPUDriver }}'
          volumeMounts:
            - name: mlnx-ofed-usr-src
              mountPath: /run/mellanox/drivers/usr/src
              mountPropagation: HostToContainer
            - name: mlnx-ofed-lib-modules
              mountPath: /run/mellanox/drivers/lib/modules
              mountPropagation: HostToContainer
            - name: run-nvidia
              mountPath: /run/nvidia/drivers
              mountPropagation: HostToContainer
          startupProbe:
            exec:
              command:
                [sh, -c, 'ls /.driver-ready']
            initialDelaySeconds: 10
            failureThreshold: 60
            successThreshold: 1
            periodSeconds: 5
          livenessProbe:
            exec:
              command:
                [sh, -c, 'lsmod | grep "nv_peer_mem\|nvidia_peermem"']
            periodSeconds: 30
            initialDelaySeconds: 30
            failureThreshold: 1
            successThreshold: 1
          readinessProbe:
            exec:
              command:
                [sh, -c, 'lsmod | grep "nv_peer_mem\|nvidia_peermem"']
            periodSeconds: 30
            initialDelaySeconds: 10
            failureThreshold: 1
      volumes:
        {{ $ofedPath := "/run/mellanox/drivers" }}
        {{ if .RuntimeSpec.UseHostOFED }}
          {{ $ofedPath = "" }}
        {{end}}
        - name: mlnx-ofed-usr-src
          hostPath:
            path: {{ $ofedPath }}/usr/src
        - name: mlnx-ofed-lib-modules
          hostPath:
            path: {{ $ofedPath }}/lib/modules
        - name: run-nvidia
          hostPath:
            path: {{ .CrSpec.GPUDriverSourcePath }}
      nodeSelector:
        feature.node.kubernetes.io/pci-15b3.present: "true"
        nvidia.com/gpu.present: "true"
        network.nvidia.com/operator.mofed.wait: "false"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.count
                operator: Gt
                values:
                  - "0"

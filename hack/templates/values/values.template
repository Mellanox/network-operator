# Copyright 2020 NVIDIA
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for network-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nfd:
  enabled: true
  deployNodeFeatureRules: true

upgradeCRDs: true

sriovNetworkOperator:
  enabled: false

# Node Feature discovery chart related values
node-feature-discovery:
  enableNodeFeatureApi: true
  worker:
    serviceAccount:
      name: node-feature-discovery
      # disable creation to avoid duplicate serviceaccount creation by master spec below
      create: false
    tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    config:
      sources:
        pci:
          deviceClassWhitelist:
          - "0300"
          - "0302"
          deviceLabelFields:
          - vendor
  master:
    serviceAccount:
      name: node-feature-discovery
      create: true
    config: 
      extraLabelNs: ["nvidia.com"]

# SR-IOV Network Operator chart related values
sriov-network-operator:
  operator:
    tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
    nodeSelector: {}
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: In
                  values: [""]
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: In
                  values: [ "" ]
    nameOverride: ""
    fullnameOverride: ""
    resourcePrefix: "nvidia.com"
    cniBinPath: "/opt/cni/bin"
    clusterType: "kubernetes"
    admissionControllers:
      enabled: false
      certificates:
        secretNames:
          operator: "operator-webhook-cert"
          injector: "network-resources-injector-cert"
        certManager:
          # When enabled, makes use of certificates managed by cert-manager.
          enabled: true
          # When enabled, certificates are generated via cert-manager and then name will match the name of the secrets
          # defined above
          generateSelfSigned: true
        # If not specified, no secret is created and secrets with the names defined above are expected to exist in the
        # cluster. In that case, the ca.crt must be base64 encoded twice since it ends up being an env variable.
        custom:
          enabled: false
      #   operator:
      #     caCrt: |
      #       -----BEGIN CERTIFICATE-----
      #       MIIMIICLDCCAdKgAwIBAgIBADAKBggqhkjOPQQDAjB9MQswCQYDVQQGEwJCRTEPMA0G
      #       ...
      #       -----END CERTIFICATE-----
      #     tlsCrt: |
      #       -----BEGIN CERTIFICATE-----
      #       MIIMIICLDCCAdKgAwIBAgIBADAKBggqhkjOPQQDAjB9MQswCQYDVQQGEwJCRTEPMA0G
      #       ...
      #       -----END CERTIFICATE-----
      #     tlsKey: |
      #       -----BEGIN EC PRIVATE KEY-----
      #       MHcl4wOuDwKQa+upc8GftXE2C//4mKANBC6It01gUaTIpo=
      #       ...
      #      -----END EC PRIVATE KEY-----
      #   injector:
      #     caCrt: |
      #       -----BEGIN CERTIFICATE-----
      #       MIIMIICLDCCAdKgAwIBAgIBADAKBggqhkjOPQQDAjB9MQswCQYDVQQGEwJCRTEPMA0G
      #       ...
      #       -----END CERTIFICATE-----
      #     tlsCrt: |
      #       -----BEGIN CERTIFICATE-----
      #       MIIMIICLDCCAdKgAwIBAgIBADAKBggqhkjOPQQDAjB9MQswCQYDVQQGEwJCRTEPMA0G
      #       ...
      #       -----END CERTIFICATE-----
      #     tlsKey: |
      #       -----BEGIN EC PRIVATE KEY-----
      #       MHcl4wOuDwKQa+upc8GftXE2C//4mKANBC6It01gUaTIpo=
      #       ...
      #      -----END EC PRIVATE KEY-----

  # Image URIs for sriov-network-operator components
  images:
    operator: {{ .SriovNetworkOperator.Repository }}/{{ .SriovNetworkOperator.Image }}:{{ .SriovNetworkOperator.Version }}
    sriovConfigDaemon: {{ .SriovConfigDaemon.Repository }}/{{ .SriovConfigDaemon.Image }}:{{ .SriovConfigDaemon.Version }}
    sriovCni: {{ .SriovCni.Repository }}/{{ .SriovCni.Image }}:{{ .SriovCni.Version }}
    ibSriovCni: {{ .SriovIbCni.Repository }}/{{ .SriovIbCni.Image }}:{{ .SriovIbCni.Version }}
    ovsCni: {{ .OVSCni.Repository }}/{{ .OVSCni.Image }}:{{ .OVSCni.Version }}
    sriovDevicePlugin: {{ .SriovDevicePlugin.Repository }}/{{ .SriovDevicePlugin.Image }}:{{ .SriovDevicePlugin.Version }}
    resourcesInjector: ghcr.io/k8snetworkplumbingwg/network-resources-injector:v1.4
    webhook: {{ .SriovNetworkOperatorWebhook.Repository }}/{{ .SriovNetworkOperatorWebhook.Image }}:{{ .SriovNetworkOperatorWebhook.Version }}
  # imagePullSecrest for SR-IOV Network Operator related images
  # imagePullSecrets: []
  sriovOperatorConfig:
    deploy: true
    configDaemonNodeSelector:
      beta.kubernetes.io/os: "linux"
      network.nvidia.com/operator.mofed.wait: "false"

# General Operator related values
# The operator element allows to deploy network operator from an alternate location
operator:
  resources:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 5m
      memory: 64Mi
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: In
                values: [""]
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/control-plane"
                operator: In
                values: [ "" ]
  repository: {{ .NetworkOperator.Repository }}
  image: {{ .NetworkOperator.Image }}
  # imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  # tag, if defined will use the given image tag, else Chart.AppVersion will be used
  # tag
  cniBinDirectory: /opt/cni/bin
  useDTK: true
  admissionController:
    enabled: false
    useCertManager: true
    # certificate:
      # tlsCrt: |
      #   -----BEGIN CERTIFICATE-----
      #   MIIMIICLDCCAdKgAwIBAgIBADAKBggqhkjOPQQDAjB9MQswCQYDVQQGEwJCRTEPMA0G
      #   ...
      #   -----END CERTIFICATE-----
      # tlsKey: |
      #   -----BEGIN EC PRIVATE KEY-----
      #   MHcl4wOuDwKQa+upc8GftXE2C//4mKANBC6It01gUaTIpo=
      #   ...
      #  -----END EC PRIVATE KEY-----

imagePullSecrets: []

# NicClusterPolicy CR values:
deployCR: false
ofedDriver:
  deploy: false
  image: {{ .Mofed.Image }}
  repository: {{ .Mofed.Repository }}
  version: {{ .Mofed.Version }}
  initContainer:
    enable: true
    repository: {{ .NetworkOperatorInitContainer.Repository }}
    image: {{ .NetworkOperatorInitContainer.Image }}
    version: {{ .NetworkOperatorInitContainer.Version }}
  # imagePullSecrets: []
  # env, if defined will pass environment variables to the OFED container
  # env:
  #   - name: EXAMPLE_ENV_VAR
  #     value: example_env_var_value
  # containerResources:
  #   - name: "mofed-container"
  #     requests:
  #       cpu: "200m"
  #       memory: "150Mi"
  #     limits:
  #       cpu: "300m"
  #       memory: "300Mi"
  terminationGracePeriodSeconds: 300
  # Private mirror repository configuration
  repoConfig:
    name: ""
  # Custom ssl key/certificate configuration
  certConfig:
    name: ""
  startupProbe:
    initialDelaySeconds: 10
    periodSeconds: 20
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 30
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 30
  upgradePolicy:
    # global switch for automatic upgrade feature
    # if set to false all other options are ignored
    autoUpgrade: true
    # how many nodes can be upgraded in parallel (default: 1)
    # 0 means no limit, all nodes will be upgraded in parallel
    maxParallelUpgrades: 1
    # cordon and drain (if enabled) a node before loading the driver on it
    safeLoad: false
    # options for node drain (`kubectl drain`) before the driver reload
    # if auto upgrade is enabled but drain.enable is false,
    # then driver POD will be reloaded immediately without
    # removing PODs from the node
    drain:
      enable: true
      force: true
      podSelector: ""
      # It's recommended to set a timeout to avoid infinite drain in case non-fatal error keeps happening on retries
      timeoutSeconds: 300
      deleteEmptyDir: true
    waitForCompletion:
      # specifies a label selector for the pods to wait for completion
      # podSelector: "app=myapp"
      # specify the length of time in seconds to wait before giving up for workload to finish, zero means infinite
      # timeoutSeconds: 300
  forcePrecompiled: false

rdmaSharedDevicePlugin:
  deploy: true
  image: {{ .RdmaSharedDevicePlugin.Image }}
  repository: {{ .RdmaSharedDevicePlugin.Repository }}
  version: {{ .RdmaSharedDevicePlugin.Version }}
  useCdi: false
  # imagePullSecrets: []
  # containerResources:
  #   - name: "rdma-shared-dp"
  #     requests:
  #       cpu: "100m"
  #       memory: "50Mi"
  #     limits:
  #       cpu: "150m"
  #       memory: "100Mi"
  # The following defines the RDMA resources in the cluster
  # it must be provided by the user when deploying the chart
  # each entry in the resources element will create a resource with the provided <name> and list of devices
  # example:
  resources:
    - name: rdma_shared_device_a
      vendors: [15b3]
      rdmaHcaMax: 63

sriovDevicePlugin:
  deploy: false
  image: {{ .SriovDevicePlugin.Image }}
  repository: {{ .SriovDevicePlugin.Repository }}
  version: {{ .SriovDevicePlugin.Version }}
  useCdi: false
  # imagePullSecrets: []
  # containerResources:
  #   - name: "kube-sriovdp"
  #     requests:
  #       cpu: "100m"
  #       memory: "50Mi"
  #     limits:
  #       cpu: "150m"
  #       memory: "100Mi"
  resources:
    - name: hostdev
      vendors: [15b3]

ibKubernetes:
  deploy: false
  image: {{ .IbKubernetes.Image }}
  repository: {{ .IbKubernetes.Repository }}
  version: {{ .IbKubernetes.Version }}
  # imagePullSecrets: []
  # containerResources:
  #   - name: "ib-kubernetes"
  #     requests:
  #       cpu: "100m"
  #       memory: "300Mi"
  #     limits:
  #       cpu: "100m"
  #       memory: "300Mi"
  periodicUpdateSeconds: 5
  pKeyGUIDPoolRangeStart: "02:00:00:00:00:00:00:00"
  pKeyGUIDPoolRangeEnd: "02:FF:FF:FF:FF:FF:FF:FF"
  ufmSecret: '' # specify the secret name here

nvIpam:
  deploy: false
  image: {{ .NvIPAM.Image }}
  repository: {{ .NvIPAM.Repository }}
  version: {{ .NvIPAM.Version }}
  enableWebhook: false
  # imagePullSecrets: []
  # containerResources:
  #   - name: "nv-ipam-node"
  #     requests:
  #       cpu: "150m"
  #       memory: "50Mi"
  #     limits:
  #       cpu: "300m"
  #       memory: "300Mi"
  #   - name: "nv-ipam-controller"
  #     requests:
  #       cpu: "150m"
  #       memory: "50Mi"
  #     limits:
  #       cpu: "300m"
  #       memory: "300Mi"

secondaryNetwork:
  deploy: true
  cniPlugins:
    deploy: true
    image: {{ .CniPlugins.Image }}
    repository: {{ .CniPlugins.Repository }}
    version: {{ .CniPlugins.Version }}
    # imagePullSecrets: []
    # containerResources:
    #   - name: "cni-plugins"
    #     requests:
    #       cpu: "100m"
    #       memory: "50Mi"
    #     limits:
    #       cpu: "100m"
    #       memory: "50Mi"
  multus:
    deploy: true
    image: {{ .Multus.Image }}
    repository: {{ .Multus.Repository }}
    version: {{ .Multus.Version }}
    # imagePullSecrets: []
    # config: ''
    # containerResources:
    #   - name: "kube-multus"
    #     requests:
    #       cpu: "100m"
    #       memory: "50Mi"
    #     limits:
    #       cpu: "100m"
    #       memory: "50Mi"
  ipoib:
    deploy: false
    image: {{ .Ipoib.Image }}
    repository: {{ .Ipoib.Repository }}
    version: {{ .Ipoib.Version }}
    # imagePullSecrets: []
    # containerResources:
    #   - name: "ipoib-cni"
    #     requests:
    #       cpu: "100m"
    #       memory: "50Mi"
    #     limits:
    #       cpu: "100m"
    #       memory: "50Mi"
  ipamPlugin:
    deploy: true
    image: {{ .IpamPlugin.Image }}
    repository: {{ .IpamPlugin.Repository }}
    version: {{ .IpamPlugin.Version }}
    # imagePullSecrets: []
    # containerResources:
    #   - name: "whereabouts"
    #     requests:
    #       cpu: "100m"
    #       memory: "100Mi"
    #     limits:
    #       cpu: "100m"
    #       memory: "200Mi"

nicFeatureDiscovery:
  deploy: false
  image: {{ .NicFeatureDiscovery.Image }}
  repository: {{ .NicFeatureDiscovery.Repository }}
  version: {{ .NicFeatureDiscovery.Version }}
  # imagePullSecrets: []
  # containerResources:
  #   - name: "nic-feature-discovery"
  #     requests:
  #       cpu: "100m"
  #       memory: "50Mi"
  #     limits:
  #       cpu: "300m"
  #       memory: "150Mi"

docaTelemetryService:
  deploy: false
  image: {{ .DOCATelemetryService.Image }}
  repository: {{ .DOCATelemetryService.Repository }}
  version: {{ .DOCATelemetryService.Version }}
  # imagePullSecrets: []
  # containerResources:
  #   - name: "doca-telemetry-service"
  #     requests:
  #       cpu: "100m"
  #       memory: "50Mi"
  #     limits:
  #       cpu: "300m"
  #       memory: "150Mi"

# Can be set to nicclusterpolicy and override other ds node affinity,
# e.g. https://github.com/Mellanox/network-operator/blob/master/manifests/state-multus-cni/0050-multus-ds.yml#L26-L36
#nodeAffinity:

# Can be set to nicclusterpolicy to add extra tolerations to ds
#tolerations:

test:
  pf: ens2f0
